# webscraping
README file on webscraping process
## what is webscraping?
webscraping, also known as web harvesting is data scraping used for extracting data from websites. It is a form of copying, in which specific data is gathered and copied from the web, typically into a central local database or spreadsheet, for later retrieval or analysis.
Web scraping a web page involves fetching it and extracting from it. Fetching is the downloading of a page.web crawling is main component of web scrapping.A web crawling, or spider, is a type of bot that's typically operated by search engines like Google and Bing.  It is a field with active developments sharing a common goal with the semantic web vision, an ambitious initiative that still requires breakthroughs in text processing, semantic understanding, artificial intelligence and human-computer interactions. 
### techniques of web scraping
there are several types of webscraping:
1.Human copy-and-paste
2.http programming
3.html parsing
4.DOM parsing 
5.vertical aggregation
6.Semantic annotation recognizing
7.Computer vision web-page analysis
8.text pattern matching
#### details of techniques in webscraping
1.Human copy-and-paste
The simplest form of web scraping is manually copying and pasting data from a web page into a text file or spreadsheet.


2.Text pattern matching
A simple yet powerful approach to extract information from web pages can be based on the UNIX grep command or regular expression-matching facilities of programming languages.

3.HTTP programming
Static and dynamic web pages can be retrieved by posting HTTP requests to the remote web server using socket programming.

4.HTML parsing
Many websites have large collections of pages generated dynamically from an underlying structured source like a database. Data of the same category are typically encoded into similar pages by a common script or template. In data mining, a program that detects such templates in a particular information source, extracts its content and translates it into a relational form, is called a wrapper. 

5.DOM parsing
Further information: Document Object Model
By embedding a full-fledged web browser, such as the Internet Explorer or the Mozilla browser control, programs can retrieve the dynamic content generated by client-side scripts. These browser controls also parse web pages into a DOM tree, based on which programs can retrieve parts of the pages. 

6.Vertical aggregation
There are several companies that have developed vertical specific harvesting platforms. These platforms create and monitor a multitude of "bots" for specific verticals with no "man in the loop" (no direct human involvement), and no work related to a specific target site. The preparation involves establishing the knowledge base for the entire vertical and then the platform creates the bots automatically. 

7.Semantic annotation recognizing
The pages being scraped may embrace metadata or semantic markups and annotations, which can be used to locate specific data snippets. If the annotations are embedded in the pages, as Microformat does, this technique can be viewed as a special case of DOM parsing.

8.Computer vision web-page analysis
There are efforts using machine learning and computer vision that attempt to identify and extract information from web pages by interpreting pages visually as a human being might.
##### Methods to prevent web scraping
The administrator of a website can use various measures to stop or slow a bot. Some techniques include:

1.Blocking an IP address either manually or based on criteria such as geolocation and DNSRBL. This will also block all browsing from that address.

2.Disabling any web service API that the website's system might expose.

3.Bots sometimes declare who they are (using user agent strings) and can be blocked on that basis using robots.txt; 'googlebot' is an example. Other bots make no distinction between themselves and a human using a browser.

4.Bots can be blocked by monitoring excess traffic

5.Bots can sometimes be blocked with tools to verify that it is a real person accessing the site, like a CAPTCHA. Bots are sometimes coded to explicitly break specific CAPTCHA patterns or may employ third-party services that utilize human labor to read and respond in real-time to CAPTCHA challenges.
